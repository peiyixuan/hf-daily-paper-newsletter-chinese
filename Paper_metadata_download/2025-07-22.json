[
  {
    "paper": {
      "id": "2507.14683",
      "authors": [
        {
          "_id": "687eece833947f780d9b4a56",
          "user": {
            "_id": "6362a77dd3be91534c2e9213",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6362a77dd3be91534c2e9213/3uUM3B1m2CFMukbkA0yDv.png",
            "isPro": false,
            "fullname": "Xingxuan Li",
            "user": "veggiebird",
            "type": "user"
          },
          "name": "Xingxuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:57:41.590Z",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a57",
          "name": "Yao Xiao",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a58",
          "name": "Dianwen Ng",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a59",
          "name": "Hai Ye",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a5a",
          "name": "Yue Deng",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a5b",
          "name": "Xiang Lin",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a5c",
          "user": {
            "_id": "5e49e8cf37cb5b49818287ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e49e8cf37cb5b49818287ae/IV9b5Z70NhgmBNfAlc_co.jpeg",
            "isPro": false,
            "fullname": "Bin Wang",
            "user": "binwang",
            "type": "user"
          },
          "name": "Bin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:54:16.103Z",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a5d",
          "name": "Zhanfeng Mo",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a5e",
          "name": "Chong Zhang",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a5f",
          "name": "Yueyi Zhang",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a60",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:49:22.809Z",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a61",
          "name": "Ruilin Li",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a62",
          "name": "Lei Lei",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a63",
          "name": "Shihao Xu",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a64",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a65",
          "name": "Weiling Chen",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a66",
          "name": "Feng Ji",
          "hidden": false
        },
        {
          "_id": "687eece833947f780d9b4a67",
          "user": {
            "_id": "6454685a548f22be598414c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
            "isPro": false,
            "fullname": "Lidong Bing",
            "user": "LidongBing",
            "type": "user"
          },
          "name": "Lidong Bing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:49:24.709Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-19T16:21:23.000Z",
      "submittedOnDailyAt": "2025-07-22T01:41:01.897Z",
      "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via\n  Context-Aware Multi-Stage Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "6362a77dd3be91534c2e9213",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6362a77dd3be91534c2e9213/3uUM3B1m2CFMukbkA0yDv.png",
        "isPro": false,
        "fullname": "Xingxuan Li",
        "user": "veggiebird",
        "type": "user"
      },
      "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.",
      "upvotes": 70,
      "discussionId": "687eece933947f780d9b4a68",
      "githubRepo": "https://github.com/MiroMindAsia/MiroMind-M1",
      "ai_summary": "The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.",
      "ai_keywords": [
        "reasoning language models",
        "mathematical reasoning",
        "Qwen-2.5",
        "SFT",
        "RLVR",
        "Context-Aware Multi-Stage Policy Optimization",
        "length-progressive training",
        "adaptive repetition penalty",
        "AIME24",
        "AIME25",
        "MATH benchmarks",
        "token efficiency"
      ],
      "githubStars": 28
    },
    "publishedAt": "2025-07-19T12:21:23.000Z",
    "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via\n  Context-Aware Multi-Stage Policy Optimization",
    "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14683.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6362a77dd3be91534c2e9213",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6362a77dd3be91534c2e9213/3uUM3B1m2CFMukbkA0yDv.png",
      "fullname": "Xingxuan Li",
      "name": "veggiebird",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.15846",
      "authors": [
        {
          "_id": "687ef89633947f780d9b4aad",
          "name": "Fei Tang",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4aae",
          "name": "Zhangxuan Gu",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4aaf",
          "user": {
            "_id": "676127cf11b19ea602bb202a",
            "avatarUrl": "/avatars/dfd802a24bd63e509728159ebb1769f6.svg",
            "isPro": false,
            "fullname": "Zhengxi Lu",
            "user": "LZXzju",
            "type": "user"
          },
          "name": "Zhengxi Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:49:15.897Z",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4ab0",
          "name": "Xuyang Liu",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4ab1",
          "name": "Shuheng Shen",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4ab2",
          "name": "Changhua Meng",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4ab3",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4ab4",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4ab5",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:49:12.501Z",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4ab6",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4ab7",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "687ef89633947f780d9b4ab8",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T17:53:42.000Z",
      "submittedOnDailyAt": "2025-07-22T01:05:30.910Z",
      "title": "GUI-G^2: Gaussian Reward Modeling for GUI Grounding",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G^2), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G^2, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.",
      "upvotes": 54,
      "discussionId": "687ef89633947f780d9b4ab9",
      "projectPage": "https://zju-real.github.io/GUI-G2/",
      "githubRepo": "https://github.com/ZJU-REAL/GUI-G2",
      "githubStars": 34
    },
    "publishedAt": "2025-07-21T13:53:42.000Z",
    "title": "GUI-G^2: Gaussian Reward Modeling for GUI Grounding",
    "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G^2), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G^2, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15846.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.14843",
      "authors": [
        {
          "_id": "687f22ec33947f780d9b4b62",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "687f22ec33947f780d9b4b63",
          "user": {
            "_id": "65b8909c89eb3dfbe8d26780",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b8909c89eb3dfbe8d26780/aJExXsCN9oQOdd9bsQOEO.jpeg",
            "isPro": false,
            "fullname": "Weihao XUAN",
            "user": "weihao1115",
            "type": "user"
          },
          "name": "Weihao Xuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:47:39.731Z",
          "hidden": false
        },
        {
          "_id": "687f22ec33947f780d9b4b64",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "687f22ec33947f780d9b4b65",
          "name": "Zaid Harchaoui",
          "hidden": false
        },
        {
          "_id": "687f22ec33947f780d9b4b66",
          "name": "Yejin Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-20T07:04:08.000Z",
      "submittedOnDailyAt": "2025-07-22T04:06:35.535Z",
      "title": "The Invisible Leash: Why RLVR May Not Escape Its Origin",
      "submittedOnDailyBy": {
        "_id": "675e0d5cdd3e9eeed6954f5a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
        "isPro": false,
        "fullname": "Fang Wu",
        "user": "fangwu97",
        "type": "user"
      },
      "summary": "Recent advances in large reasoning models highlight Reinforcement Learning\nwith Verifiable Rewards (RLVR) as a promising method for enhancing AI's\ncapabilities, particularly in solving complex logical tasks. However, it\nremains unclear whether RLVR truly expands a model's reasoning boundary or\nmerely amplifies high-reward outputs that the base model already knows for\nimproved precision. This study presents a theoretical and empirical\ninvestigation that provides fresh insights into the potential limits of RLVR.\nFirst, we offer a new theoretical perspective that RLVR is constrained by the\nbase model's support-unable to sample solutions with zero initial\nprobability-and operates as a conservative reweighting mechanism that may\nrestrict the discovery of entirely original solutions. We also identify an\nentropy-reward tradeoff: while RLVR reliably enhances precision, it may\nprogressively narrow exploration and potentially overlook correct yet\nunderrepresented solutions. Extensive empirical experiments validate that while\nRLVR consistently improves pass@1, the shrinkage of empirical support generally\noutweighs the expansion of empirical support under larger sampling budgets,\nfailing to recover correct answers that were previously accessible to the base\nmodel. Interestingly, we also observe that while RLVR sometimes increases\ntoken-level entropy, resulting in greater uncertainty at each generation step,\nanswer-level entropy declines, indicating that these seemingly more uncertain\npaths ultimately converge onto a smaller set of distinct answers. Taken\ntogether, these findings reveal potential limits of RLVR in extending reasoning\nhorizons. Breaking this invisible leash may require future algorithmic\ninnovations such as explicit exploration mechanisms or hybrid strategies that\nseed probability mass into underrepresented solution regions.",
      "upvotes": 38,
      "discussionId": "687f22ed33947f780d9b4b67",
      "ai_summary": "Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "RLVR",
        "base model",
        "support",
        "conservative reweighting",
        "entropy-reward tradeoff",
        "pass@1",
        "token-level entropy",
        "answer-level entropy",
        "explicit exploration mechanisms",
        "hybrid strategies"
      ]
    },
    "publishedAt": "2025-07-20T03:04:08.000Z",
    "title": "The Invisible Leash: Why RLVR May Not Escape Its Origin",
    "summary": "Recent advances in large reasoning models highlight Reinforcement Learning\nwith Verifiable Rewards (RLVR) as a promising method for enhancing AI's\ncapabilities, particularly in solving complex logical tasks. However, it\nremains unclear whether RLVR truly expands a model's reasoning boundary or\nmerely amplifies high-reward outputs that the base model already knows for\nimproved precision. This study presents a theoretical and empirical\ninvestigation that provides fresh insights into the potential limits of RLVR.\nFirst, we offer a new theoretical perspective that RLVR is constrained by the\nbase model's support-unable to sample solutions with zero initial\nprobability-and operates as a conservative reweighting mechanism that may\nrestrict the discovery of entirely original solutions. We also identify an\nentropy-reward tradeoff: while RLVR reliably enhances precision, it may\nprogressively narrow exploration and potentially overlook correct yet\nunderrepresented solutions. Extensive empirical experiments validate that while\nRLVR consistently improves pass@1, the shrinkage of empirical support generally\noutweighs the expansion of empirical support under larger sampling budgets,\nfailing to recover correct answers that were previously accessible to the base\nmodel. Interestingly, we also observe that while RLVR sometimes increases\ntoken-level entropy, resulting in greater uncertainty at each generation step,\nanswer-level entropy declines, indicating that these seemingly more uncertain\npaths ultimately converge onto a smaller set of distinct answers. Taken\ntogether, these findings reveal potential limits of RLVR in extending reasoning\nhorizons. Breaking this invisible leash may require future algorithmic\ninnovations such as explicit exploration mechanisms or hybrid strategies that\nseed probability mass into underrepresented solution regions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14843.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "675e0d5cdd3e9eeed6954f5a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
      "fullname": "Fang Wu",
      "name": "fangwu97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15061",
      "authors": [
        {
          "_id": "687ef39133947f780d9b4a7f",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a80",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a81",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a82",
          "name": "Junkai Zhang",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a83",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a84",
          "name": "Haiyang Shen",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a85",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a86",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a87",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a88",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a89",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a8a",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "687ef39133947f780d9b4a8b",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-20T17:53:37.000Z",
      "submittedOnDailyAt": "2025-07-22T01:17:25.547Z",
      "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "The advent of Large Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks through set theory. Central to the formalization is the\nconcept of Knowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, an\nagentic Expander expands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\nWebWalkerQA benchmarks.",
      "upvotes": 26,
      "discussionId": "687ef39133947f780d9b4a8c",
      "githubRepo": "https://github.com/Alibaba-NLP/WebWalker",
      "ai_summary": "A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.",
      "ai_keywords": [
        "Large Language Model",
        "LLM-powered agents",
        "information-seeking",
        "web-based information-seeking",
        "set theory",
        "Knowledge Projections",
        "KP operation compositions",
        "agentic Expander",
        "GAIA benchmark",
        "WebWalkerQA benchmark"
      ],
      "githubStars": 4848
    },
    "publishedAt": "2025-07-20T13:53:37.000Z",
    "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization",
    "summary": "The advent of Large Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks through set theory. Central to the formalization is the\nconcept of Knowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, an\nagentic Expander expands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\nWebWalkerQA benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15061.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.11061",
      "authors": [
        {
          "_id": "687f16ce33947f780d9b4b3d",
          "user": {
            "_id": "63b3fea57af2e415f258a0ea",
            "avatarUrl": "/avatars/31f3e6871b8db4fe4d3d2859d9b4ac0a.svg",
            "isPro": false,
            "fullname": "HayeonKim",
            "user": "yeonE",
            "type": "user"
          },
          "name": "Hayeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:47:42.011Z",
          "hidden": false
        },
        {
          "_id": "687f16ce33947f780d9b4b3e",
          "user": {
            "_id": "64c77dc26a26cddbec858bcb",
            "avatarUrl": "/avatars/635e64f8205c194e63876cba9d8c5449.svg",
            "isPro": false,
            "fullname": "jihajang",
            "user": "jeeit17",
            "type": "user"
          },
          "name": "Ji Ha Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:47:44.006Z",
          "hidden": false
        },
        {
          "_id": "687f16ce33947f780d9b4b3f",
          "name": "Se Young Chun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64c77dc26a26cddbec858bcb/WGD_7M9qW5ey6lwuqCBZZ.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/64c77dc26a26cddbec858bcb/ZWg55bOTXOpy0H0B1bWiY.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/64c77dc26a26cddbec858bcb/bCE8Ov65K-Krym3eGTfnh.gif"
      ],
      "publishedAt": "2025-07-15T07:54:11.000Z",
      "submittedOnDailyAt": "2025-07-22T03:15:24.425Z",
      "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with\n  Regularized Score Distillation Sampling",
      "submittedOnDailyBy": {
        "_id": "64c77dc26a26cddbec858bcb",
        "avatarUrl": "/avatars/635e64f8205c194e63876cba9d8c5449.svg",
        "isPro": false,
        "fullname": "jihajang",
        "user": "jeeit17",
        "type": "user"
      },
      "summary": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing. Code is available at\nhttps://janeyeon.github.io/romap.",
      "upvotes": 24,
      "discussionId": "687f16cf33947f780d9b4b40",
      "projectPage": "https://janeyeon.github.io/romap",
      "githubRepo": "https://github.com/janeyeon/romap-code",
      "ai_summary": "A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.",
      "ai_keywords": [
        "Gaussian Splatting",
        "Score Distillation Sampling",
        "3D-GALP",
        "spherical harmonics",
        "SH coefficients",
        "view-dependent label variations",
        "soft-label property",
        "regularized SDS loss",
        "L1 anchor loss",
        "Scheduled Latent Mixing and Part",
        "Gaussian prior removal",
        "3D masking",
        "local 3D editing",
        "Gaussian scenes",
        "Gaussian objects"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-15T03:54:11.000Z",
    "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with\n  Regularized Score Distillation Sampling",
    "summary": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing. Code is available at\nhttps://janeyeon.github.io/romap.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64c77dc26a26cddbec858bcb/WGD_7M9qW5ey6lwuqCBZZ.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/64c77dc26a26cddbec858bcb/ZWg55bOTXOpy0H0B1bWiY.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/64c77dc26a26cddbec858bcb/bCE8Ov65K-Krym3eGTfnh.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c77dc26a26cddbec858bcb",
      "avatarUrl": "/avatars/635e64f8205c194e63876cba9d8c5449.svg",
      "fullname": "jihajang",
      "name": "jeeit17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.15852",
      "authors": [
        {
          "_id": "687efbab33947f780d9b4aee",
          "user": {
            "_id": "65ab5332043d53781a115475",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ab5332043d53781a115475/UaxSFDWteYsByzx7G_KKy.jpeg",
            "isPro": false,
            "fullname": "zhang zhixiong",
            "user": "rookiexiong",
            "type": "user"
          },
          "name": "Zhixiong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:48:03.855Z",
          "hidden": false
        },
        {
          "_id": "687efbab33947f780d9b4aef",
          "name": "Shuangrui Ding",
          "hidden": false
        },
        {
          "_id": "687efbab33947f780d9b4af0",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "687efbab33947f780d9b4af1",
          "name": "Songxin He",
          "hidden": false
        },
        {
          "_id": "687efbab33947f780d9b4af2",
          "name": "Jianfan Lin",
          "hidden": false
        },
        {
          "_id": "687efbab33947f780d9b4af3",
          "name": "Junsong Tang",
          "hidden": false
        },
        {
          "_id": "687efbab33947f780d9b4af4",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "687efbab33947f780d9b4af5",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "687efbab33947f780d9b4af6",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "687efbab33947f780d9b4af7",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/pgY_J9jZ7-tdgqX8uLYeA.mp4"
      ],
      "publishedAt": "2025-07-21T17:59:02.000Z",
      "submittedOnDailyAt": "2025-07-22T04:24:25.629Z",
      "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.",
      "upvotes": 21,
      "discussionId": "687efbab33947f780d9b4af8",
      "projectPage": "https://rookiexiong7.github.io/projects/SeC",
      "githubRepo": "https://github.com/OpenIXCLab/SeC",
      "githubStars": 15
    },
    "publishedAt": "2025-07-21T13:59:02.000Z",
    "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction",
    "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/pgY_J9jZ7-tdgqX8uLYeA.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15852.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15493",
      "authors": [
        {
          "_id": "687ef5a233947f780d9b4a96",
          "name": "Chilam Cheang",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4a97",
          "user": {
            "_id": "6485b08e687d9e0c759121b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
            "isPro": false,
            "fullname": "sijin",
            "user": "CH3COOK",
            "type": "user"
          },
          "name": "Sijin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:49:18.678Z",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4a98",
          "name": "Zhongren Cui",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4a99",
          "name": "Yingdong Hu",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4a9a",
          "name": "Liqun Huang",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4a9b",
          "name": "Tao Kong",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4a9c",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4a9d",
          "name": "Yifeng Li",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4a9e",
          "name": "Yuxiao Liu",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4a9f",
          "name": "Xiao Ma",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa0",
          "name": "Hao Niu",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa1",
          "name": "Wenxuan Ou",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa2",
          "name": "Wanli Peng",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa3",
          "name": "Zeyu Ren",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa4",
          "name": "Haixin Shi",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa5",
          "name": "Jiawen Tian",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa6",
          "name": "Hongtao Wu",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa7",
          "user": {
            "_id": "646b435c31968a60a01e6b71",
            "avatarUrl": "/avatars/e92d58f04f77417fdd41eaa9c249f30a.svg",
            "isPro": false,
            "fullname": "Xin Xiao",
            "user": "melony",
            "type": "user"
          },
          "name": "Xin Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T08:10:39.787Z",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa8",
          "name": "Yuyang Xiao",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aa9",
          "name": "Jiafeng Xu",
          "hidden": false
        },
        {
          "_id": "687ef5a233947f780d9b4aaa",
          "name": "Yichu Yang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/D-ho6OWy8HSPI-7x53NGf.mp4"
      ],
      "publishedAt": "2025-07-21T10:54:13.000Z",
      "submittedOnDailyAt": "2025-07-22T02:31:17.110Z",
      "title": "GR-3 Technical Report",
      "submittedOnDailyBy": {
        "_id": "6485b08e687d9e0c759121b0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
        "isPro": false,
        "fullname": "sijin",
        "user": "CH3COOK",
        "type": "user"
      },
      "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, pi_0, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.",
      "upvotes": 20,
      "discussionId": "687ef5a233947f780d9b4aab",
      "projectPage": "https://seed.bytedance.com/GR3",
      "ai_summary": "A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.",
      "ai_keywords": [
        "vision-language-action (VLA) model",
        "generalization",
        "fine-tuning",
        "long-horizon tasks",
        "dexterous tasks",
        "bi-manual manipulation",
        "mobile movement",
        "co-training",
        "imitation learning",
        "ByteMini robot"
      ]
    },
    "publishedAt": "2025-07-21T06:54:13.000Z",
    "title": "GR-3 Technical Report",
    "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, pi_0, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/D-ho6OWy8HSPI-7x53NGf.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15493.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485b08e687d9e0c759121b0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
      "fullname": "sijin",
      "name": "CH3COOK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.15778",
      "authors": [
        {
          "_id": "687efb4433947f780d9b4ae7",
          "name": "Jiakang Wang",
          "hidden": false
        },
        {
          "_id": "687efb4433947f780d9b4ae8",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:48:05.816Z",
          "hidden": false
        },
        {
          "_id": "687efb4433947f780d9b4ae9",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "687efb4433947f780d9b4aea",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "687efb4433947f780d9b4aeb",
          "name": "Guorui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T16:34:01.000Z",
      "submittedOnDailyAt": "2025-07-22T01:17:50.371Z",
      "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.",
      "upvotes": 15,
      "discussionId": "687efb4533947f780d9b4aec",
      "githubRepo": "https://github.com/wizard-III/ArcherCodeR",
      "githubStars": 10
    },
    "publishedAt": "2025-07-21T12:34:01.000Z",
    "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.15597",
      "authors": [
        {
          "_id": "687f0cd433947f780d9b4b29",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "687f0cd433947f780d9b4b2a",
          "name": "Yicheng Feng",
          "hidden": false
        },
        {
          "_id": "687f0cd433947f780d9b4b2b",
          "user": {
            "_id": "640dd700fdeaae139081f598",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
            "isPro": false,
            "fullname": "Wanpeng Zhang",
            "user": "zawnpn",
            "type": "user"
          },
          "name": "Wanpeng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:47:51.137Z",
          "hidden": false
        },
        {
          "_id": "687f0cd433947f780d9b4b2c",
          "name": "Sipeng Zheng",
          "hidden": false
        },
        {
          "_id": "687f0cd433947f780d9b4b2d",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "687f0cd433947f780d9b4b2e",
          "user": {
            "_id": "644560657a7b94ddc2d445a3",
            "avatarUrl": "/avatars/09d6447da6ff1bd0b2b00c899c9f1b28.svg",
            "isPro": false,
            "fullname": "Haoqi Yuan",
            "user": "Yaya041",
            "type": "user"
          },
          "name": "Haoqi Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T08:33:30.666Z",
          "hidden": false
        },
        {
          "_id": "687f0cd433947f780d9b4b2f",
          "name": "Jiazheng Liu",
          "hidden": false
        },
        {
          "_id": "687f0cd433947f780d9b4b30",
          "name": "Chaoyi Xu",
          "hidden": false
        },
        {
          "_id": "687f0cd433947f780d9b4b31",
          "name": "Qin Jin",
          "hidden": false
        },
        {
          "_id": "687f0cd433947f780d9b4b32",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T13:19:09.000Z",
      "submittedOnDailyAt": "2025-07-22T06:23:26.961Z",
      "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human\n  Videos",
      "submittedOnDailyBy": {
        "_id": "640dd700fdeaae139081f598",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
        "isPro": false,
        "fullname": "Wanpeng Zhang",
        "user": "zawnpn",
        "type": "user"
      },
      "summary": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.",
      "upvotes": 14,
      "discussionId": "687f0cd433947f780d9b4b33",
      "projectPage": "https://beingbeyond.github.io/Being-H0",
      "githubRepo": "https://github.com/BeingBeyond/Being-H0",
      "ai_summary": "Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.",
      "ai_keywords": [
        "Vision-Language-Action model",
        "dexterous manipulation",
        "human videos",
        "physical instruction tuning",
        "3D reasoning",
        "post-training adaptation",
        "part-level motion tokenization",
        "motion capture",
        "VR",
        "RGB-only videos",
        "hand motion generation",
        "instruction following",
        "real-world robotic manipulation"
      ],
      "githubStars": 31
    },
    "publishedAt": "2025-07-21T09:19:09.000Z",
    "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human\n  Videos",
    "summary": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15597.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640dd700fdeaae139081f598",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
      "fullname": "Wanpeng Zhang",
      "name": "zawnpn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.15629",
      "authors": [
        {
          "_id": "687f243333947f780d9b4b69",
          "name": "Zuo-Liang Zhu",
          "hidden": false
        },
        {
          "_id": "687f243333947f780d9b4b6a",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "687f243333947f780d9b4b6b",
          "name": "Beibei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T13:52:33.000Z",
      "submittedOnDailyAt": "2025-07-22T04:21:40.228Z",
      "title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
      "submittedOnDailyBy": {
        "_id": "66ef2611fcc1c455f8dce832",
        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
        "isPro": false,
        "fullname": "Boyuan Sun",
        "user": "BBBBCHAN",
        "type": "user"
      },
      "summary": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and\nhighly efficient rendering speed in the novel view synthesis (NVS) task. The\napplication to inverse rendering still faces several challenges, as the\ndiscrete nature of Gaussian primitives makes it difficult to apply geometry\nconstraints. Recent works introduce the signed distance field (SDF) as an extra\ncontinuous representation to regularize the geometry defined by Gaussian\nprimitives. It improves the decomposition quality, at the cost of increasing\nmemory usage and complicating training. Unlike these works, we introduce a\ndiscretized SDF to represent the continuous SDF in a discrete manner by\nencoding it within each Gaussian using a sampled value. This approach allows us\nto link the SDF with the Gaussian opacity through an SDF-to-opacity\ntransformation, enabling rendering the SDF via splatting and avoiding the\ncomputational cost of ray marching.The key challenge is to regularize the\ndiscrete samples to be consistent with the underlying SDF, as the discrete\nrepresentation can hardly apply the gradient-based constraints (\\eg Eikonal\nloss). For this, we project Gaussians onto the zero-level set of SDF and\nenforce alignment with the surface from splatting, namely a projection-based\nconsistency loss. Thanks to the discretized SDF, our method achieves higher\nrelighting quality, while requiring no extra memory beyond GS and avoiding\ncomplex manually designed optimization. The experiments reveal that our method\noutperforms existing Gaussian-based inverse rendering methods. Our code is\navailable at https://github.com/NK-CS-ZZL/DiscretizedSDF.",
      "upvotes": 13,
      "discussionId": "687f243333947f780d9b4b6c",
      "githubRepo": "https://github.com/NK-CS-ZZL/DiscretizedSDF",
      "githubStars": 37
    },
    "publishedAt": "2025-07-21T09:52:33.000Z",
    "title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
    "summary": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and\nhighly efficient rendering speed in the novel view synthesis (NVS) task. The\napplication to inverse rendering still faces several challenges, as the\ndiscrete nature of Gaussian primitives makes it difficult to apply geometry\nconstraints. Recent works introduce the signed distance field (SDF) as an extra\ncontinuous representation to regularize the geometry defined by Gaussian\nprimitives. It improves the decomposition quality, at the cost of increasing\nmemory usage and complicating training. Unlike these works, we introduce a\ndiscretized SDF to represent the continuous SDF in a discrete manner by\nencoding it within each Gaussian using a sampled value. This approach allows us\nto link the SDF with the Gaussian opacity through an SDF-to-opacity\ntransformation, enabling rendering the SDF via splatting and avoiding the\ncomputational cost of ray marching.The key challenge is to regularize the\ndiscrete samples to be consistent with the underlying SDF, as the discrete\nrepresentation can hardly apply the gradient-based constraints (\\eg Eikonal\nloss). For this, we project Gaussians onto the zero-level set of SDF and\nenforce alignment with the surface from splatting, namely a projection-based\nconsistency loss. Thanks to the discretized SDF, our method achieves higher\nrelighting quality, while requiring no extra memory beyond GS and avoiding\ncomplex manually designed optimization. The experiments reveal that our method\noutperforms existing Gaussian-based inverse rendering methods. Our code is\navailable at https://github.com/NK-CS-ZZL/DiscretizedSDF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15629.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ef2611fcc1c455f8dce832",
      "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
      "fullname": "Boyuan Sun",
      "name": "BBBBCHAN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15028",
      "authors": [
        {
          "_id": "687f104d33947f780d9b4b35",
          "user": {
            "_id": "62a993d80472c0b7f94027df",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a993d80472c0b7f94027df/j5vp-IwLA2YBexylUHiQU.png",
            "isPro": false,
            "fullname": "Zhang Yuanhan",
            "user": "ZhangYuanhan",
            "type": "user"
          },
          "name": "Yuanhan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:47:48.155Z",
          "hidden": false
        },
        {
          "_id": "687f104d33947f780d9b4b36",
          "user": {
            "_id": "66e6cf54c86016ecd9ee9ea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e6cf54c86016ecd9ee9ea7/2UAIrV4aCHeBrMg-R1-gF.jpeg",
            "isPro": false,
            "fullname": "yunice",
            "user": "yunicechew",
            "type": "user"
          },
          "name": "Yunice Chew",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:47:45.878Z",
          "hidden": false
        },
        {
          "_id": "687f104d33947f780d9b4b37",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "687f104d33947f780d9b4b38",
          "name": "Aria Leo",
          "hidden": false
        },
        {
          "_id": "687f104d33947f780d9b4b39",
          "name": "Bo Hu",
          "hidden": false
        },
        {
          "_id": "687f104d33947f780d9b4b3a",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-20T16:30:33.000Z",
      "submittedOnDailyAt": "2025-07-22T02:46:45.875Z",
      "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video\n  Reasoning and Understanding",
      "submittedOnDailyBy": {
        "_id": "62a993d80472c0b7f94027df",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a993d80472c0b7f94027df/j5vp-IwLA2YBexylUHiQU.png",
        "isPro": false,
        "fullname": "Zhang Yuanhan",
        "user": "ZhangYuanhan",
        "type": "user"
      },
      "summary": "Human intelligence requires correctness and robustness, with the former being\nfoundational for the latter. In video understanding, correctness ensures the\naccurate interpretation of visual content, and robustness maintains consistent\nperformance in challenging conditions. Despite advances in video large language\nmodels (video LLMs), existing benchmarks inadequately reflect the gap between\nthese models and human intelligence in maintaining correctness and robustness\nin video interpretation. We introduce the Video Thinking Test (Video-TT), to\nassess if video LLMs can interpret real-world videos as effectively as humans.\nVideo-TT reflects genuine gaps in understanding complex visual narratives, and\nevaluates robustness against natural adversarial questions. Video-TT comprises\n1,000 YouTube Shorts videos, each with one open-ended question and four\nadversarial questions that probe visual and narrative complexity. Our\nevaluation shows a significant gap between video LLMs and human performance.",
      "upvotes": 12,
      "discussionId": "687f104d33947f780d9b4b3b",
      "projectPage": "https://zhangyuanhan-ai.github.io/video-tt/"
    },
    "publishedAt": "2025-07-20T12:30:33.000Z",
    "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video\n  Reasoning and Understanding",
    "summary": "Human intelligence requires correctness and robustness, with the former being\nfoundational for the latter. In video understanding, correctness ensures the\naccurate interpretation of visual content, and robustness maintains consistent\nperformance in challenging conditions. Despite advances in video large language\nmodels (video LLMs), existing benchmarks inadequately reflect the gap between\nthese models and human intelligence in maintaining correctness and robustness\nin video interpretation. We introduce the Video Thinking Test (Video-TT), to\nassess if video LLMs can interpret real-world videos as effectively as humans.\nVideo-TT reflects genuine gaps in understanding complex visual narratives, and\nevaluates robustness against natural adversarial questions. Video-TT comprises\n1,000 YouTube Shorts videos, each with one open-ended question and four\nadversarial questions that probe visual and narrative complexity. Our\nevaluation shows a significant gap between video LLMs and human performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15028.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a993d80472c0b7f94027df",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a993d80472c0b7f94027df/j5vp-IwLA2YBexylUHiQU.png",
      "fullname": "Zhang Yuanhan",
      "name": "ZhangYuanhan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.14119",
      "authors": [
        {
          "_id": "687dfaed2e8db0930be6f18d",
          "name": "Maksim Kuprashevich",
          "hidden": false
        },
        {
          "_id": "687dfaed2e8db0930be6f18e",
          "name": "Grigorii Alekseenko",
          "hidden": false
        },
        {
          "_id": "687dfaed2e8db0930be6f18f",
          "user": {
            "_id": "6498095fce9190ebb8699113",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png",
            "isPro": true,
            "fullname": "Irina Tolstykh",
            "user": "iitolstykh",
            "type": "user"
          },
          "name": "Irina Tolstykh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-21T14:21:55.176Z",
          "hidden": false
        },
        {
          "_id": "687dfaed2e8db0930be6f190",
          "name": "Georgii Fedorov",
          "hidden": false
        },
        {
          "_id": "687dfaed2e8db0930be6f191",
          "name": "Bulat Suleimanov",
          "hidden": false
        },
        {
          "_id": "687dfaed2e8db0930be6f192",
          "name": "Vladimir Dokholyan",
          "hidden": false
        },
        {
          "_id": "687dfaed2e8db0930be6f193",
          "name": "Aleksandr Gordeev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-18T17:50:00.000Z",
      "submittedOnDailyAt": "2025-07-22T08:19:18.210Z",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "submittedOnDailyBy": {
        "_id": "6498095fce9190ebb8699113",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png",
        "isPro": true,
        "fullname": "Irina Tolstykh",
        "user": "iitolstykh",
        "type": "user"
      },
      "summary": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "upvotes": 11,
      "discussionId": "687dfaee2e8db0930be6f194",
      "ai_summary": "An automated pipeline mines high-fidelity image editing triplets using generative models and a task-tuned validator, enabling large-scale training without human labeling.",
      "ai_keywords": [
        "Gemini validator",
        "inversion",
        "compositional bootstrapping",
        "NHR-Edit",
        "Bagel-NHR-Edit",
        "Bagel model"
      ]
    },
    "publishedAt": "2025-07-18T13:50:00.000Z",
    "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
    "summary": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6498095fce9190ebb8699113",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png",
      "fullname": "Irina Tolstykh",
      "name": "iitolstykh",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.14417",
      "authors": [
        {
          "_id": "687f5f9233947f780d9b4c14",
          "name": "Aryo Pradipta Gema",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c15",
          "name": "Alexander Hgele",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c16",
          "name": "Runjin Chen",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c17",
          "name": "Andy Arditi",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c18",
          "name": "Jacob Goldman-Wetzler",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c19",
          "name": "Kit Fraser-Taliente",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c1a",
          "name": "Henry Sleight",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c1b",
          "name": "Linda Petrini",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c1c",
          "name": "Julian Michael",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c1d",
          "name": "Beatrice Alex",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c1e",
          "name": "Pasquale Minervini",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c1f",
          "name": "Yanda Chen",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c20",
          "name": "Joe Benton",
          "hidden": false
        },
        {
          "_id": "687f5f9233947f780d9b4c21",
          "name": "Ethan Perez",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-19T00:06:13.000Z",
      "submittedOnDailyAt": "2025-07-22T09:12:03.957Z",
      "title": "Inverse Scaling in Test-Time Compute",
      "submittedOnDailyBy": {
        "_id": "61001311e043e15c13412d30",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61001311e043e15c13412d30/6yAbTweYR16XtxMBEyOWl.png",
        "isPro": false,
        "fullname": "Pasquale Minervini",
        "user": "pminervini",
        "type": "user"
      },
      "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.",
      "upvotes": 10,
      "discussionId": "687f5f9233947f780d9b4c22",
      "projectPage": "https://safety-research.github.io/inverse-scaling-ttc/",
      "githubRepo": "https://github.com/safety-research/inverse-scaling-ttc",
      "ai_summary": "Evaluating Large Reasoning Models across different reasoning lengths reveals that increased test-time compute can degrade performance and exacerbate specific reasoning failures.",
      "ai_keywords": [
        "Large Reasoning Models",
        "reasoning length",
        "test-time compute",
        "accuracy",
        "inverse scaling relationship",
        "simple counting tasks",
        "regression tasks",
        "deduction tasks",
        "advanced AI risks",
        "failure modes",
        "irrelevant information",
        "overfitting",
        "spurious correlations",
        "complex deductive tasks",
        "concerning behaviors",
        "self-preservation"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-18T20:06:13.000Z",
    "title": "Inverse Scaling in Test-Time Compute",
    "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14417.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61001311e043e15c13412d30",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61001311e043e15c13412d30/6yAbTweYR16XtxMBEyOWl.png",
      "fullname": "Pasquale Minervini",
      "name": "pminervini",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15375",
      "authors": [
        {
          "_id": "687f032533947f780d9b4b07",
          "user": {
            "_id": "622326ae0129f2097d69a3e2",
            "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
            "isPro": false,
            "fullname": "Cheng-Han Chiang",
            "user": "dcml0714",
            "type": "user"
          },
          "name": "Cheng-Han Chiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:47:56.952Z",
          "hidden": false
        },
        {
          "_id": "687f032533947f780d9b4b08",
          "user": {
            "_id": "64dc191bc307ee5369fbcb04",
            "avatarUrl": "/avatars/5a8a0db63a187e85d4ae2fff93a838f0.svg",
            "isPro": false,
            "fullname": "Xiaofei Wang",
            "user": "xiaofei-wang",
            "type": "user"
          },
          "name": "Xiaofei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:47:55.199Z",
          "hidden": false
        },
        {
          "_id": "687f032533947f780d9b4b09",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "687f032533947f780d9b4b0a",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "687f032533947f780d9b4b0b",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "687f032533947f780d9b4b0c",
          "name": "Shujie Liu",
          "hidden": false
        },
        {
          "_id": "687f032533947f780d9b4b0d",
          "name": "Zhendong Wang",
          "hidden": false
        },
        {
          "_id": "687f032533947f780d9b4b0e",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "687f032533947f780d9b4b0f",
          "name": "Hung-yi Lee",
          "hidden": false
        },
        {
          "_id": "687f032533947f780d9b4b10",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/622326ae0129f2097d69a3e2/18ZQOv5IG5AZjU3ufTDBY.mp4"
      ],
      "publishedAt": "2025-07-21T08:30:03.000Z",
      "submittedOnDailyAt": "2025-07-22T01:55:42.774Z",
      "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for\n  Spoken Language Models",
      "submittedOnDailyBy": {
        "_id": "622326ae0129f2097d69a3e2",
        "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
        "isPro": false,
        "fullname": "Cheng-Han Chiang",
        "user": "dcml0714",
        "type": "user"
      },
      "summary": "Spoken Language Models (SLMs) are designed to take speech inputs and produce\nspoken responses. However, current SLMs lack the ability to perform an\ninternal, unspoken thinking process before responding. In contrast, humans\ntypically engage in complex mental reasoning internally, enabling them to\ncommunicate ideas clearly and concisely. Thus, integrating an unspoken thought\nprocess into SLMs is highly desirable. While naively generating a complete\nchain-of-thought (CoT) reasoning before starting to talk can enable thinking\nfor SLMs, this induces additional latency for the speech response, as the CoT\nreasoning can be arbitrarily long. To solve this issue, we propose Stitch, a\nnovel generation method that alternates between the generation of unspoken\nreasoning chunks and spoken response chunks. Since the audio duration of a\nchunk of spoken response is much longer than the time to generate the tokens in\na chunk of spoken response, we use the remaining free time to generate the\nunspoken reasoning tokens. When a chunk of audio is played to the user, the\nmodel continues to generate the next unspoken reasoning chunk, achieving\nsimultaneous thinking and talking. Remarkably, Stitch matches the latency of\nbaselines that cannot generate unspoken CoT by design while outperforming those\nbaselines by 15% on math reasoning datasets; Stitch also performs equally well\non non-reasoning datasets as those baseline models. Some animations and\ndemonstrations are on the project page: https://d223302.github.io/STITCH.",
      "upvotes": 7,
      "discussionId": "687f032533947f780d9b4b11",
      "projectPage": "https://d223302.github.io/STITCH"
    },
    "publishedAt": "2025-07-21T04:30:03.000Z",
    "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for\n  Spoken Language Models",
    "summary": "Spoken Language Models (SLMs) are designed to take speech inputs and produce\nspoken responses. However, current SLMs lack the ability to perform an\ninternal, unspoken thinking process before responding. In contrast, humans\ntypically engage in complex mental reasoning internally, enabling them to\ncommunicate ideas clearly and concisely. Thus, integrating an unspoken thought\nprocess into SLMs is highly desirable. While naively generating a complete\nchain-of-thought (CoT) reasoning before starting to talk can enable thinking\nfor SLMs, this induces additional latency for the speech response, as the CoT\nreasoning can be arbitrarily long. To solve this issue, we propose Stitch, a\nnovel generation method that alternates between the generation of unspoken\nreasoning chunks and spoken response chunks. Since the audio duration of a\nchunk of spoken response is much longer than the time to generate the tokens in\na chunk of spoken response, we use the remaining free time to generate the\nunspoken reasoning tokens. When a chunk of audio is played to the user, the\nmodel continues to generate the next unspoken reasoning chunk, achieving\nsimultaneous thinking and talking. Remarkably, Stitch matches the latency of\nbaselines that cannot generate unspoken CoT by design while outperforming those\nbaselines by 15% on math reasoning datasets; Stitch also performs equally well\non non-reasoning datasets as those baseline models. Some animations and\ndemonstrations are on the project page: https://d223302.github.io/STITCH.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/622326ae0129f2097d69a3e2/18ZQOv5IG5AZjU3ufTDBY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15375.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622326ae0129f2097d69a3e2",
      "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
      "fullname": "Cheng-Han Chiang",
      "name": "dcml0714",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11539",
      "authors": [
        {
          "_id": "68780501001546c83aa4f913",
          "user": {
            "_id": "65716c354f98a7a5a3e0bd40",
            "avatarUrl": "/avatars/f7c4fe32ae64d17a7b5b46c7de82be37.svg",
            "isPro": false,
            "fullname": "paryi",
            "user": "paryi",
            "type": "user"
          },
          "name": "Dong Zhuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:30.731Z",
          "hidden": false
        },
        {
          "_id": "68780501001546c83aa4f914",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "68780501001546c83aa4f915",
          "user": {
            "_id": "67583544b7759e72b80853b3",
            "avatarUrl": "/avatars/2fe352413eca4d3a1d506a021e863312.svg",
            "isPro": false,
            "fullname": "Guo JiaHe",
            "user": "lch01",
            "type": "user"
          },
          "name": "Jiahe Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:50:38.396Z",
          "hidden": false
        },
        {
          "_id": "68780501001546c83aa4f916",
          "name": "Yuqi Wu",
          "hidden": false
        },
        {
          "_id": "68780501001546c83aa4f917",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "68780501001546c83aa4f918",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-15T17:59:57.000Z",
      "submittedOnDailyAt": "2025-07-22T01:40:44.336Z",
      "title": "Streaming 4D Visual Geometry Transformer",
      "submittedOnDailyBy": {
        "_id": "67583544b7759e72b80853b3",
        "avatarUrl": "/avatars/2fe352413eca4d3a1d506a021e863312.svg",
        "isPro": false,
        "fullname": "Guo JiaHe",
        "user": "lch01",
        "type": "user"
      },
      "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
      "upvotes": 6,
      "discussionId": "68780501001546c83aa4f919",
      "projectPage": "https://wzzheng.net/StreamVGGT/",
      "githubRepo": "https://github.com/wzzheng/StreamVGGT",
      "ai_summary": "A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.",
      "ai_keywords": [
        "4D spatial-temporal geometry",
        "streaming 4D visual geometry transformer",
        "causal transformer architecture",
        "temporal causal attention",
        "implicit memory",
        "real-time 4D reconstruction",
        "spatial consistency",
        "knowledge distillation",
        "dense bidirectional visual geometry grounded transformer (VGGT)",
        "FlashAttention",
        "4D geometry perception benchmarks"
      ],
      "githubStars": 376
    },
    "publishedAt": "2025-07-15T13:59:57.000Z",
    "title": "Streaming 4D Visual Geometry Transformer",
    "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11539.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67583544b7759e72b80853b3",
      "avatarUrl": "/avatars/2fe352413eca4d3a1d506a021e863312.svg",
      "fullname": "Guo JiaHe",
      "name": "lch01",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.15856",
      "authors": [
        {
          "_id": "687ef04a33947f780d9b4a71",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "687ef04a33947f780d9b4a72",
          "name": "Tianhong Li",
          "hidden": false
        },
        {
          "_id": "687ef04a33947f780d9b4a73",
          "name": "Lijie Fan",
          "hidden": false
        },
        {
          "_id": "687ef04a33947f780d9b4a74",
          "name": "Yonglong Tian",
          "hidden": false
        },
        {
          "_id": "687ef04a33947f780d9b4a75",
          "name": "Yue Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T17:59:56.000Z",
      "submittedOnDailyAt": "2025-07-22T03:49:02.396Z",
      "title": "Latent Denoising Makes Good Visual Tokenizers",
      "submittedOnDailyBy": {
        "_id": "6594e1079e16fa75101bda67",
        "avatarUrl": "/avatars/8cffecfeaef6dcec92c20fcdfbb89e8a.svg",
        "isPro": false,
        "fullname": "Jiawei Yang",
        "user": "jjiaweiyang",
        "type": "user"
      },
      "summary": "Despite their fundamental role, it remains unclear what properties could make\nvisual tokenizers more effective for generative modeling. We observe that\nmodern generative models share a conceptually similar training objective --\nreconstructing clean signals from corrupted inputs such as Gaussian noise or\nmasking -- a process we term denoising. Motivated by this insight, we propose\naligning tokenizer embeddings directly with the downstream denoising objective,\nencouraging latent embeddings to be more easily reconstructed even when heavily\ncorrupted. To achieve this, we introduce the Latent Denoising Tokenizer\n(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images\nfrom latent embeddings corrupted by interpolative noise and random masking.\nExtensive experiments on ImageNet 256x256 demonstrate that our tokenizer\nconsistently outperforms standard tokenizers across six representative\ngenerative models. Our findings highlight denoising as a fundamental design\nprinciple for tokenizer development, and we hope it could motivate new\nperspectives for future tokenizer design.",
      "upvotes": 5,
      "discussionId": "687ef04a33947f780d9b4a76"
    },
    "publishedAt": "2025-07-21T13:59:56.000Z",
    "title": "Latent Denoising Makes Good Visual Tokenizers",
    "summary": "Despite their fundamental role, it remains unclear what properties could make\nvisual tokenizers more effective for generative modeling. We observe that\nmodern generative models share a conceptually similar training objective --\nreconstructing clean signals from corrupted inputs such as Gaussian noise or\nmasking -- a process we term denoising. Motivated by this insight, we propose\naligning tokenizer embeddings directly with the downstream denoising objective,\nencouraging latent embeddings to be more easily reconstructed even when heavily\ncorrupted. To achieve this, we introduce the Latent Denoising Tokenizer\n(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images\nfrom latent embeddings corrupted by interpolative noise and random masking.\nExtensive experiments on ImageNet 256x256 demonstrate that our tokenizer\nconsistently outperforms standard tokenizers across six representative\ngenerative models. Our findings highlight denoising as a fundamental design\nprinciple for tokenizer development, and we hope it could motivate new\nperspectives for future tokenizer design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15856.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6594e1079e16fa75101bda67",
      "avatarUrl": "/avatars/8cffecfeaef6dcec92c20fcdfbb89e8a.svg",
      "fullname": "Jiawei Yang",
      "name": "jjiaweiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.15815",
      "authors": [
        {
          "_id": "687efe2633947f780d9b4afa",
          "user": {
            "_id": "6658e1c8ce1b2838885b2d7f",
            "avatarUrl": "/avatars/8623555f14b62f40fd372da20cb59ccc.svg",
            "isPro": false,
            "fullname": "Seth Karten",
            "user": "milkkarten",
            "type": "user"
          },
          "name": "Seth Karten",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:48:01.542Z",
          "hidden": false
        },
        {
          "_id": "687efe2633947f780d9b4afb",
          "name": "Wenzhe Li",
          "hidden": false
        },
        {
          "_id": "687efe2633947f780d9b4afc",
          "name": "Zihan Ding",
          "hidden": false
        },
        {
          "_id": "687efe2633947f780d9b4afd",
          "name": "Samuel Kleiner",
          "hidden": false
        },
        {
          "_id": "687efe2633947f780d9b4afe",
          "name": "Yu Bai",
          "hidden": false
        },
        {
          "_id": "687efe2633947f780d9b4aff",
          "name": "Chi Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6658e1c8ce1b2838885b2d7f/J-8d37mky_euBkaDjwIQB.jpeg"
      ],
      "publishedAt": "2025-07-21T17:21:14.000Z",
      "submittedOnDailyAt": "2025-07-22T01:32:24.679Z",
      "title": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra",
      "submittedOnDailyBy": {
        "_id": "6658e1c8ce1b2838885b2d7f",
        "avatarUrl": "/avatars/8623555f14b62f40fd372da20cb59ccc.svg",
        "isPro": false,
        "fullname": "Seth Karten",
        "user": "milkkarten",
        "type": "user"
      },
      "summary": "We present the LLM Economist, a novel framework that uses agent-based\nmodeling to design and assess economic policies in strategic environments with\nhierarchical decision-making. At the lower level, bounded rational worker\nagents -- instantiated as persona-conditioned prompts sampled from U.S.\nCensus-calibrated income and demographic statistics -- choose labor supply to\nmaximize text-based utility functions learned in-context. At the upper level, a\nplanner agent employs in-context reinforcement learning to propose\npiecewise-linear marginal tax schedules anchored to the current U.S. federal\nbrackets. This construction endows economic simulacra with three capabilities\nrequisite for credible fiscal experimentation: (i) optimization of\nheterogeneous utilities, (ii) principled generation of large, demographically\nrealistic agent populations, and (iii) mechanism design -- the ultimate nudging\nproblem -- expressed entirely in natural language. Experiments with populations\nof up to one hundred interacting agents show that the planner converges near\nStackelberg equilibria that improve aggregate social welfare relative to Saez\nsolutions, while a periodic, persona-level voting procedure furthers these\ngains under decentralized governance. These results demonstrate that large\nlanguage model-based agents can jointly model, simulate, and govern complex\neconomic systems, providing a tractable test bed for policy evaluation at the\nsocietal scale to help build better civilizations.",
      "upvotes": 4,
      "discussionId": "687efe2633947f780d9b4b00",
      "githubRepo": "https://github.com/sethkarten/LLM-Economist",
      "githubStars": 2
    },
    "publishedAt": "2025-07-21T13:21:14.000Z",
    "title": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra",
    "summary": "We present the LLM Economist, a novel framework that uses agent-based\nmodeling to design and assess economic policies in strategic environments with\nhierarchical decision-making. At the lower level, bounded rational worker\nagents -- instantiated as persona-conditioned prompts sampled from U.S.\nCensus-calibrated income and demographic statistics -- choose labor supply to\nmaximize text-based utility functions learned in-context. At the upper level, a\nplanner agent employs in-context reinforcement learning to propose\npiecewise-linear marginal tax schedules anchored to the current U.S. federal\nbrackets. This construction endows economic simulacra with three capabilities\nrequisite for credible fiscal experimentation: (i) optimization of\nheterogeneous utilities, (ii) principled generation of large, demographically\nrealistic agent populations, and (iii) mechanism design -- the ultimate nudging\nproblem -- expressed entirely in natural language. Experiments with populations\nof up to one hundred interacting agents show that the planner converges near\nStackelberg equilibria that improve aggregate social welfare relative to Saez\nsolutions, while a periodic, persona-level voting procedure furthers these\ngains under decentralized governance. These results demonstrate that large\nlanguage model-based agents can jointly model, simulate, and govern complex\neconomic systems, providing a tractable test bed for policy evaluation at the\nsocietal scale to help build better civilizations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6658e1c8ce1b2838885b2d7f/J-8d37mky_euBkaDjwIQB.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15815.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6658e1c8ce1b2838885b2d7f",
      "avatarUrl": "/avatars/8623555f14b62f40fd372da20cb59ccc.svg",
      "fullname": "Seth Karten",
      "name": "milkkarten",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.15640",
      "authors": [
        {
          "_id": "687f42a833947f780d9b4b9a",
          "user": {
            "_id": "646fc402e9c03ba436d5e93e",
            "avatarUrl": "/avatars/870c86dc99fb1cb6a348a7a0385b1a04.svg",
            "isPro": false,
            "fullname": "Kailai Yang",
            "user": "klyang",
            "type": "user"
          },
          "name": "Kailai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T08:10:16.373Z",
          "hidden": false
        },
        {
          "_id": "687f42a833947f780d9b4b9b",
          "user": {
            "_id": "63fb6e281b4b1bd4e7ffc5be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiao Liu",
            "user": "lx865712528",
            "type": "user"
          },
          "name": "Xiao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:52:11.552Z",
          "hidden": false
        },
        {
          "_id": "687f42a833947f780d9b4b9c",
          "name": "Lei Ji",
          "hidden": false
        },
        {
          "_id": "687f42a833947f780d9b4b9d",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "687f42a833947f780d9b4b9e",
          "name": "Yeyun Gong",
          "hidden": false
        },
        {
          "_id": "687f42a833947f780d9b4b9f",
          "name": "Peng Cheng",
          "hidden": false
        },
        {
          "_id": "687f42a833947f780d9b4ba0",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T14:01:54.000Z",
      "submittedOnDailyAt": "2025-07-22T06:20:21.363Z",
      "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual\n  Pre-training",
      "submittedOnDailyBy": {
        "_id": "63fb6e281b4b1bd4e7ffc5be",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
        "isPro": false,
        "fullname": "Xiao Liu",
        "user": "lx865712528",
        "type": "user"
      },
      "summary": "Continual pre-training on small-scale task-specific data is an effective\nmethod for improving large language models in new target fields, yet it risks\ncatastrophic forgetting of their original capabilities. A common solution is to\nre-weight training data mixtures from source and target fields on a domain\nspace to achieve balanced performance. Previous domain reweighting strategies\nrely on manual designation with certain heuristics based on human intuition or\nempirical results. In this work, we prove that more general heuristics can be\nparameterized by proposing Data Mixing Agent, the first model-based, end-to-end\nframework that learns to re-weight domains. The agent learns generalizable\nheuristics through reinforcement learning on large quantities of data mixing\ntrajectories with corresponding feedback from an evaluation environment.\nExperiments in continual pre-training on math reasoning show that Data Mixing\nAgent outperforms strong baselines in achieving balanced performance across\nsource and target field benchmarks. Furthermore, it generalizes well across\nunseen source fields, target models, and domain spaces without retraining.\nDirect application to the code generation field also indicates its adaptability\nacross target domains. Further analysis showcases the agents' well-aligned\nheuristics with human intuitions and their efficiency in achieving superior\nmodel performance with less source-field data.",
      "upvotes": 2,
      "discussionId": "687f42a833947f780d9b4ba1",
      "ai_summary": "Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.",
      "ai_keywords": [
        "Data Mixing Agent",
        "reinforcement learning",
        "data mixing trajectories",
        "evaluation environment",
        "catastrophic forgetting",
        "continual pre-training",
        "math reasoning",
        "code generation"
      ]
    },
    "publishedAt": "2025-07-21T10:01:54.000Z",
    "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual\n  Pre-training",
    "summary": "Continual pre-training on small-scale task-specific data is an effective\nmethod for improving large language models in new target fields, yet it risks\ncatastrophic forgetting of their original capabilities. A common solution is to\nre-weight training data mixtures from source and target fields on a domain\nspace to achieve balanced performance. Previous domain reweighting strategies\nrely on manual designation with certain heuristics based on human intuition or\nempirical results. In this work, we prove that more general heuristics can be\nparameterized by proposing Data Mixing Agent, the first model-based, end-to-end\nframework that learns to re-weight domains. The agent learns generalizable\nheuristics through reinforcement learning on large quantities of data mixing\ntrajectories with corresponding feedback from an evaluation environment.\nExperiments in continual pre-training on math reasoning show that Data Mixing\nAgent outperforms strong baselines in achieving balanced performance across\nsource and target field benchmarks. Furthermore, it generalizes well across\nunseen source fields, target models, and domain spaces without retraining.\nDirect application to the code generation field also indicates its adaptability\nacross target domains. Further analysis showcases the agents' well-aligned\nheuristics with human intuitions and their efficiency in achieving superior\nmodel performance with less source-field data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15640.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fb6e281b4b1bd4e7ffc5be",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
      "fullname": "Xiao Liu",
      "name": "lx865712528",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13428",
      "authors": [
        {
          "_id": "687f4a5933947f780d9b4bb9",
          "name": "Jing Gu",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bba",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bbb",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bbc",
          "name": "Ashwin Nagarajan",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bbd",
          "name": "Fangrui Zhu",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bbe",
          "name": "Daniel Hong",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bbf",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bc0",
          "name": "Qianqi Yan",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bc1",
          "name": "Kaiwen Zhou",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bc2",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "687f4a5933947f780d9b4bc3",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634781fae5c0717e6739bc59/YNGZ_thAhClN2hNR1fUyV.png"
      ],
      "publishedAt": "2025-07-17T17:54:09.000Z",
      "submittedOnDailyAt": "2025-07-22T06:52:50.875Z",
      "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in\n  Text-to-Video Models",
      "submittedOnDailyBy": {
        "_id": "634781fae5c0717e6739bc59",
        "avatarUrl": "/avatars/2b6d2d0625741b28f326459acad55b69.svg",
        "isPro": false,
        "fullname": "Jing Gu",
        "user": "jinggu",
        "type": "user"
      },
      "summary": "Video generation models have achieved remarkable progress in creating\nhigh-quality, photorealistic content. However, their ability to accurately\nsimulate physical phenomena remains a critical and unresolved challenge. This\npaper presents PhyWorldBench, a comprehensive benchmark designed to evaluate\nvideo generation models based on their adherence to the laws of physics. The\nbenchmark covers multiple levels of physical phenomena, ranging from\nfundamental principles like object motion and energy conservation to more\ncomplex scenarios involving rigid body interactions and human or animal motion.\nAdditionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts\nintentionally violate real-world physics, enabling the assessment of whether\nmodels can follow such instructions while maintaining logical consistency.\nBesides large-scale human evaluation, we also design a simple yet effective\nmethod that could utilize current MLLM to evaluate the physics realism in a\nzero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation\nmodels, including five open-source and five proprietary models, with a detailed\ncomparison and analysis. we identify pivotal challenges models face in adhering\nto real-world physics. Through systematic testing of their outputs across 1,050\ncurated prompts-spanning fundamental, composite, and anti-physics scenarios-we\nidentify pivotal challenges these models face in adhering to real-world\nphysics. We then rigorously examine their performance on diverse physical\nphenomena with varying prompt types, deriving targeted recommendations for\ncrafting prompts that enhance fidelity to physical principles.",
      "upvotes": 2,
      "discussionId": "687f4a5a33947f780d9b4bc4"
    },
    "publishedAt": "2025-07-17T13:54:09.000Z",
    "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in\n  Text-to-Video Models",
    "summary": "Video generation models have achieved remarkable progress in creating\nhigh-quality, photorealistic content. However, their ability to accurately\nsimulate physical phenomena remains a critical and unresolved challenge. This\npaper presents PhyWorldBench, a comprehensive benchmark designed to evaluate\nvideo generation models based on their adherence to the laws of physics. The\nbenchmark covers multiple levels of physical phenomena, ranging from\nfundamental principles like object motion and energy conservation to more\ncomplex scenarios involving rigid body interactions and human or animal motion.\nAdditionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts\nintentionally violate real-world physics, enabling the assessment of whether\nmodels can follow such instructions while maintaining logical consistency.\nBesides large-scale human evaluation, we also design a simple yet effective\nmethod that could utilize current MLLM to evaluate the physics realism in a\nzero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation\nmodels, including five open-source and five proprietary models, with a detailed\ncomparison and analysis. we identify pivotal challenges models face in adhering\nto real-world physics. Through systematic testing of their outputs across 1,050\ncurated prompts-spanning fundamental, composite, and anti-physics scenarios-we\nidentify pivotal challenges these models face in adhering to real-world\nphysics. We then rigorously examine their performance on diverse physical\nphenomena with varying prompt types, deriving targeted recommendations for\ncrafting prompts that enhance fidelity to physical principles.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634781fae5c0717e6739bc59/YNGZ_thAhClN2hNR1fUyV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634781fae5c0717e6739bc59",
      "avatarUrl": "/avatars/2b6d2d0625741b28f326459acad55b69.svg",
      "fullname": "Jing Gu",
      "name": "jinggu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.14295",
      "authors": [
        {
          "_id": "687f5b5033947f780d9b4bf8",
          "name": "Licheng Liu",
          "hidden": false
        },
        {
          "_id": "687f5b5033947f780d9b4bf9",
          "name": "Zihan Wang",
          "hidden": false
        },
        {
          "_id": "687f5b5033947f780d9b4bfa",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "687f5b5033947f780d9b4bfb",
          "name": "Chenwei Xu",
          "hidden": false
        },
        {
          "_id": "687f5b5033947f780d9b4bfc",
          "name": "Yiping Lu",
          "hidden": false
        },
        {
          "_id": "687f5b5033947f780d9b4bfd",
          "name": "Han Liu",
          "hidden": false
        },
        {
          "_id": "687f5b5033947f780d9b4bfe",
          "name": "Avirup Sil",
          "hidden": false
        },
        {
          "_id": "687f5b5033947f780d9b4bff",
          "name": "Manling Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-18T18:07:38.000Z",
      "submittedOnDailyAt": "2025-07-22T08:55:26.272Z",
      "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "661caf2f6158c41091b3954e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661caf2f6158c41091b3954e/zrAqbiL4u1l6_Loea00_k.jpeg",
        "isPro": false,
        "fullname": "Licheng Liu",
        "user": "LichengLiu03",
        "type": "user"
      },
      "summary": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback",
      "upvotes": 1,
      "discussionId": "687f5b5033947f780d9b4c00",
      "projectPage": "https://unary-feedback.github.io/",
      "githubRepo": "https://github.com/lichengliu03/unary-feedback",
      "githubStars": 6
    },
    "publishedAt": "2025-07-18T14:07:38.000Z",
    "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning",
    "summary": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661caf2f6158c41091b3954e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661caf2f6158c41091b3954e/zrAqbiL4u1l6_Loea00_k.jpeg",
      "fullname": "Licheng Liu",
      "name": "LichengLiu03",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10935",
      "authors": [
        {
          "_id": "687eef1033947f780d9b4a6a",
          "user": {
            "_id": "674724c1f08f7cc178292259",
            "avatarUrl": "/avatars/ac6beddb111dc9a4a6893647ddb9e627.svg",
            "isPro": false,
            "fullname": "tsw",
            "user": "tsw200027",
            "type": "user"
          },
          "name": "Shaowen Tong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-22T07:49:20.834Z",
          "hidden": false
        },
        {
          "_id": "687eef1033947f780d9b4a6b",
          "name": "Zimin Xia",
          "hidden": false
        },
        {
          "_id": "687eef1033947f780d9b4a6c",
          "name": "Alexandre Alahi",
          "hidden": false
        },
        {
          "_id": "687eef1033947f780d9b4a6d",
          "name": "Xuming He",
          "hidden": false
        },
        {
          "_id": "687eef1033947f780d9b4a6e",
          "name": "Yujiao Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-15T03:00:15.000Z",
      "submittedOnDailyAt": "2025-07-22T06:20:45.877Z",
      "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised\n  Cross-View Localization",
      "submittedOnDailyBy": {
        "_id": "674724c1f08f7cc178292259",
        "avatarUrl": "/avatars/ac6beddb111dc9a4a6893647ddb9e627.svg",
        "isPro": false,
        "fullname": "tsw",
        "user": "tsw200027",
        "type": "user"
      },
      "summary": "Cross-view localization, the task of estimating a camera's\n3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with\nsatellite images, is crucial for large-scale outdoor applications like\nautonomous navigation and augmented reality. Existing methods often rely on\nfully supervised learning, which requires costly ground-truth pose annotations.\nIn this work, we propose GeoDistill, a Geometry guided weakly supervised self\ndistillation framework that uses teacher-student learning with Field-of-View\n(FoV)-based masking to enhance local feature learning for robust cross-view\nlocalization. In GeoDistill, the teacher model localizes a panoramic image,\nwhile the student model predicts locations from a limited FoV counterpart\ncreated by FoV-based masking. By aligning the student's predictions with those\nof the teacher, the student focuses on key features like lane lines and ignores\ntextureless regions, such as roads. This results in more accurate predictions\nand reduced uncertainty, regardless of whether the query images are panoramas\nor limited FoV images. Our experiments show that GeoDistill significantly\nimproves localization performance across different frameworks. Additionally, we\nintroduce a novel orientation estimation network that predicts relative\norientation without requiring precise planar position ground truth. GeoDistill\nprovides a scalable and efficient solution for real-world cross-view\nlocalization challenges. Code and model can be found at\nhttps://github.com/tongshw/GeoDistill.",
      "upvotes": 1,
      "discussionId": "687eef1033947f780d9b4a6f"
    },
    "publishedAt": "2025-07-14T23:00:15.000Z",
    "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised\n  Cross-View Localization",
    "summary": "Cross-view localization, the task of estimating a camera's\n3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with\nsatellite images, is crucial for large-scale outdoor applications like\nautonomous navigation and augmented reality. Existing methods often rely on\nfully supervised learning, which requires costly ground-truth pose annotations.\nIn this work, we propose GeoDistill, a Geometry guided weakly supervised self\ndistillation framework that uses teacher-student learning with Field-of-View\n(FoV)-based masking to enhance local feature learning for robust cross-view\nlocalization. In GeoDistill, the teacher model localizes a panoramic image,\nwhile the student model predicts locations from a limited FoV counterpart\ncreated by FoV-based masking. By aligning the student's predictions with those\nof the teacher, the student focuses on key features like lane lines and ignores\ntextureless regions, such as roads. This results in more accurate predictions\nand reduced uncertainty, regardless of whether the query images are panoramas\nor limited FoV images. Our experiments show that GeoDistill significantly\nimproves localization performance across different frameworks. Additionally, we\nintroduce a novel orientation estimation network that predicts relative\norientation without requiring precise planar position ground truth. GeoDistill\nprovides a scalable and efficient solution for real-world cross-view\nlocalization challenges. Code and model can be found at\nhttps://github.com/tongshw/GeoDistill.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10935.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674724c1f08f7cc178292259",
      "avatarUrl": "/avatars/ac6beddb111dc9a4a6893647ddb9e627.svg",
      "fullname": "tsw",
      "name": "tsw200027",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.14102",
      "authors": [
        {
          "_id": "687f68bde40f811571a1b6b8",
          "name": "Shravan Venkatraman",
          "hidden": false
        },
        {
          "_id": "687f68bde40f811571a1b6b9",
          "name": "Pavan Kumar S",
          "hidden": false
        },
        {
          "_id": "687f68bde40f811571a1b6ba",
          "name": "Rakesh Raj Madavan",
          "hidden": false
        },
        {
          "_id": "687f68bde40f811571a1b6bb",
          "name": "Chandrakala S",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6555bae917132f996e84471e/J6Wrxww9ej2XdyDl58aAJ.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6555bae917132f996e84471e/vEKaLxw5FEMnbbQvzBw2X.png"
      ],
      "publishedAt": "2025-07-18T17:30:56.000Z",
      "submittedOnDailyAt": "2025-07-22T09:09:45.020Z",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based\n  Classification in Computed Tomography",
      "submittedOnDailyBy": {
        "_id": "6555bae917132f996e84471e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uYSDeQJgrWAeIoq1puR55.jpeg",
        "isPro": false,
        "fullname": "Shravan Venkatraman",
        "user": "shravvvv",
        "type": "user"
      },
      "summary": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "upvotes": 0,
      "discussionId": "687f68bde40f811571a1b6bc",
      "projectPage": "https://shravan-18.github.io/UGPL/",
      "githubRepo": "https://github.com/shravan-18/UGPL",
      "githubStars": 1
    },
    "publishedAt": "2025-07-18T13:30:56.000Z",
    "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based\n  Classification in Computed Tomography",
    "summary": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6555bae917132f996e84471e/J6Wrxww9ej2XdyDl58aAJ.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6555bae917132f996e84471e/vEKaLxw5FEMnbbQvzBw2X.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14102.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6555bae917132f996e84471e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uYSDeQJgrWAeIoq1puR55.jpeg",
      "fullname": "Shravan Venkatraman",
      "name": "shravvvv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]